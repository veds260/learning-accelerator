{
  "cards": [
    {
      "id": "1",
      "front": "Why do we need tokenization in LLMs?",
      "back": "Neural networks can only process numbers, not raw text. Tokenization converts text into numeric IDs that the model can work with. It also reduces vocabulary size compared to character-level encoding while maintaining semantic meaning better than word-level encoding.",
      "concept": "tokenization",
      "tier": "foundation",
      "manningChapter": "2.2"
    },
    {
      "id": "2",
      "front": "What is Byte Pair Encoding (BPE) and why is it used?",
      "back": "BPE is a tokenization algorithm that iteratively merges the most frequent pairs of characters/tokens to build a vocabulary. It balances between character-level (too long) and word-level (too large vocab) encoding. Used in GPT models because it handles rare words well and keeps vocab size manageable (~50k tokens).",
      "concept": "tokenization",
      "tier": "foundation",
      "manningChapter": "2.5"
    },
    {
      "id": "3",
      "front": "What are special tokens like <|endoftext|> and why do we need them?",
      "back": "Special tokens mark boundaries and special contexts. <|endoftext|> separates different documents during training. <|unk|> represents unknown/rare tokens. They help the model understand document structure and handle edge cases.",
      "concept": "tokenization",
      "tier": "foundation",
      "manningChapter": "2.4"
    },
    {
      "id": "4",
      "front": "What is the sliding window technique for creating training data?",
      "back": "Sliding window creates overlapping sequences from text by moving a fixed-size window forward by a stride. For example, with window=4 and stride=1: 'ABCDEFG' becomes (ABCD→E), (BCDE→F), (CDEF→G). This efficiently generates many training examples from limited text.",
      "concept": "data-preparation",
      "tier": "foundation",
      "manningChapter": "2.6"
    },
    {
      "id": "5",
      "front": "What is the purpose of token embeddings?",
      "back": "Token embeddings convert discrete token IDs into continuous vector representations. Similar tokens get similar vectors, capturing semantic relationships (e.g., 'king' and 'queen' are closer than 'king' and 'car'). This learned representation enables the model to understand meaning.",
      "concept": "embeddings",
      "tier": "foundation",
      "manningChapter": "2.7"
    },
    {
      "id": "6",
      "front": "Why do we need positional encodings in transformers?",
      "back": "Transformers process all tokens in parallel, so they have no inherent sense of order (unlike RNNs). Positional encodings add position information to embeddings, allowing the model to know that 'cat sat' and 'sat cat' are different. Without them, all permutations would be treated identically.",
      "concept": "embeddings",
      "tier": "foundation",
      "manningChapter": "2.8"
    },
    {
      "id": "7",
      "front": "Explain self-attention in one sentence.",
      "back": "Self-attention allows each token to 'look at' all other tokens in the sequence and decide how much to focus on each one when computing its representation.",
      "concept": "attention",
      "tier": "intermediate",
      "manningChapter": "3.3"
    },
    {
      "id": "8",
      "front": "What are Query, Key, and Value in attention?",
      "back": "Query (Q): 'what I'm looking for' - the current token's search vector. Key (K): 'what I offer' - each token's indexable content. Value (V): 'what I contain' - the actual information to retrieve. Attention computes Q·K to find relevance, then retrieves V.",
      "concept": "attention",
      "tier": "intermediate",
      "manningChapter": "3.4"
    },
    {
      "id": "9",
      "front": "Write the scaled dot-product attention formula.",
      "back": "Attention(Q, K, V) = softmax(QK^T / √d_k) V\n\nWhere d_k is the dimension of keys. The √d_k scaling prevents softmax saturation when dimensions are large.",
      "concept": "attention",
      "tier": "intermediate",
      "manningChapter": "3.4"
    },
    {
      "id": "10",
      "front": "What is a causal mask and why is it essential for GPT?",
      "back": "Causal mask is an upper triangular matrix that prevents tokens from attending to future tokens. Essential for autoregressive generation - during training, token i can only see tokens 1...i, not i+1...n. This ensures the model learns to predict next tokens without 'cheating'.",
      "concept": "attention",
      "tier": "intermediate",
      "manningChapter": "3.5.1"
    },
    {
      "id": "11",
      "front": "Why use dropout on attention weights?",
      "back": "Dropout randomly zeros some attention weights during training, forcing the model to learn redundant attention patterns. This prevents overfitting and makes the model more robust. Typical rate: 0.1-0.3 for attention.",
      "concept": "attention",
      "tier": "intermediate",
      "manningChapter": "3.5.2"
    },
    {
      "id": "12",
      "front": "What is multi-head attention and what's the benefit?",
      "back": "Multi-head attention runs multiple attention mechanisms in parallel (e.g., 12 heads), each learning different patterns. One head might focus on syntax, another on semantics. Outputs are concatenated and projected. Benefit: model learns richer, more diverse relationships than single-head attention.",
      "concept": "multi-head-attention",
      "tier": "intermediate",
      "manningChapter": "3.6"
    },
    {
      "id": "13",
      "front": "How are embeddings split across attention heads?",
      "back": "If d_model=768 and num_heads=12, each head gets d_k = 768/12 = 64 dimensions. The embedding is split, not copied. Each head computes attention on its slice, then all outputs are concatenated back to 768 dimensions.",
      "concept": "multi-head-attention",
      "tier": "intermediate",
      "manningChapter": "3.6.2"
    },
    {
      "id": "14",
      "front": "What is Layer Normalization and why is it used in transformers?",
      "back": "LayerNorm normalizes activations across the feature dimension (not batch like BatchNorm). It stabilizes training, enables higher learning rates, and works well with variable sequence lengths. Applied before/after each sub-layer in transformers.",
      "concept": "architecture",
      "tier": "advanced",
      "manningChapter": "4.2"
    },
    {
      "id": "15",
      "front": "What is GELU activation and how does it differ from ReLU?",
      "back": "GELU (Gaussian Error Linear Unit) is a smooth activation: x * Φ(x) where Φ is CDF of standard normal. Unlike ReLU (hard cutoff at 0), GELU is smooth and allows small negative values. Used in GPT/BERT because smoothness helps gradient flow.",
      "concept": "architecture",
      "tier": "advanced",
      "manningChapter": "4.3"
    },
    {
      "id": "16",
      "front": "Why are residual connections critical in deep transformers?",
      "back": "Residual connections (x + F(x)) create gradient highways, allowing gradients to flow directly through many layers. Without them, deep networks suffer from vanishing gradients. They also help preserve positional information through the network.",
      "concept": "architecture",
      "tier": "advanced",
      "manningChapter": "4.4"
    },
    {
      "id": "17",
      "front": "What is the structure of a transformer block?",
      "back": "Standard GPT block: \n1. LayerNorm → Multi-Head Attention → Residual Add\n2. LayerNorm → FeedForward (2-layer MLP with GELU) → Residual Add\n\nThis is 'pre-norm' architecture. Post-norm puts LayerNorm after residual.",
      "concept": "architecture",
      "tier": "advanced",
      "manningChapter": "4.5"
    },
    {
      "id": "18",
      "front": "What's the typical size of the feedforward layer in GPT?",
      "back": "FFN typically has 4× hidden dimension: d_model → 4*d_model → d_model. For GPT-2 small (d_model=768), FFN is 768 → 3072 → 768. This is where most parameters live (~2/3 of total).",
      "concept": "architecture",
      "tier": "advanced",
      "manningChapter": "4.3"
    },
    {
      "id": "19",
      "front": "How many parameters does GPT-2 small have?",
      "back": "124 million parameters. Breakdown: embeddings (~38M), 12 transformer layers (~85M), final layer norm + head (~1M). Calculation: roughly 12 * (768^2 * 12 for attn + 768*3072*2 for FFN).",
      "concept": "architecture",
      "tier": "advanced",
      "manningChapter": "4.6"
    },
    {
      "id": "20",
      "front": "What is the language modeling head?",
      "back": "The final linear layer that projects from d_model to vocab_size, producing logits for each token in the vocabulary. These logits are converted to probabilities via softmax for next-token prediction.",
      "concept": "architecture",
      "tier": "advanced",
      "manningChapter": "4.6"
    },
    {
      "id": "21",
      "front": "Explain autoregressive text generation.",
      "back": "Generate one token at a time, conditioning each new token on all previous tokens. Process: 1) Input context → model → logits, 2) Sample next token from logits, 3) Append to context, 4) Repeat. Stops at max length or <|endoftext|>.",
      "concept": "generation",
      "tier": "advanced",
      "manningChapter": "4.7"
    },
    {
      "id": "22",
      "front": "What does temperature do in sampling?",
      "back": "Temperature T scales logits before softmax: softmax(logits/T). T<1 makes distribution sharper (more confident/repetitive). T>1 makes it flatter (more random/creative). T=1 is unchanged. Typical range: 0.7-1.5.",
      "concept": "generation",
      "tier": "expert",
      "manningChapter": "5.3.1"
    },
    {
      "id": "23",
      "front": "What is top-k sampling?",
      "back": "Sample only from the k most probable tokens, redistributing their probabilities. E.g., k=40 means consider only top 40 tokens, ignore the rest. Prevents sampling low-probability nonsense while maintaining diversity. Fixed k can be limiting (sometimes top 40 is too many, sometimes too few).",
      "concept": "generation",
      "tier": "expert",
      "manningChapter": "5.3.2"
    },
    {
      "id": "24",
      "front": "What is top-p (nucleus) sampling and why is it better than top-k?",
      "back": "Sample from the smallest set of tokens whose cumulative probability ≥ p (e.g., p=0.9). Adaptive: uses fewer tokens when model is confident, more when uncertain. Generally better than top-k because it adjusts to probability distribution shape.",
      "concept": "generation",
      "tier": "expert",
      "manningChapter": "5.3.2"
    },
    {
      "id": "25",
      "front": "What loss function is used for language model training?",
      "back": "Cross-entropy loss between predicted token probabilities and true next tokens. For each position, we compute -log(P(correct_token)). Average over all positions and sequences. Minimizing this maximizes likelihood of training data.",
      "concept": "training",
      "tier": "expert",
      "manningChapter": "5.1.2"
    },
    {
      "id": "26",
      "front": "Why split data into training and validation sets?",
      "back": "Training set: used to update weights. Validation set: held-out data to measure generalization. If train loss decreases but val loss increases, the model is overfitting (memorizing rather than learning patterns). Val loss guides early stopping.",
      "concept": "training",
      "tier": "expert",
      "manningChapter": "5.1.3"
    },
    {
      "id": "27",
      "front": "What is learning rate warmup and why use it?",
      "back": "Gradually increase learning rate from near-zero to target over first N steps (e.g., 2000 steps). Prevents large weight updates early in training when gradients are noisy. Helps training stability, especially for large models.",
      "concept": "training",
      "tier": "expert",
      "manningChapter": "5.2"
    },
    {
      "id": "28",
      "front": "What optimizer is typically used for training LLMs and why?",
      "back": "AdamW (Adam with decoupled weight decay). Combines Adam's adaptive learning rates with proper L2 regularization. Better than vanilla Adam for transformers. Typical hyperparams: lr=3e-4, betas=(0.9, 0.95), weight_decay=0.1.",
      "concept": "training",
      "tier": "expert",
      "manningChapter": "5.2"
    },
    {
      "id": "29",
      "front": "How do you save and load PyTorch model checkpoints?",
      "back": "Save: torch.save({'model': model.state_dict(), 'optimizer': optimizer.state_dict(), 'epoch': epoch}, 'checkpoint.pt'). Load: checkpoint = torch.load('checkpoint.pt'); model.load_state_dict(checkpoint['model']). Always save optimizer state to resume training correctly.",
      "concept": "model-management",
      "tier": "expert",
      "manningChapter": "5.4"
    },
    {
      "id": "30",
      "front": "What's the difference between pretraining and fine-tuning?",
      "back": "Pretraining: train on massive unlabeled text (e.g., web pages) to learn language. Fine-tuning: continue training on small labeled dataset for specific task. Fine-tuning is much faster (hours vs weeks) and achieves better task performance than training from scratch.",
      "concept": "finetuning",
      "tier": "mastery",
      "manningChapter": "6.1"
    },
    {
      "id": "31",
      "front": "How do you add a classification head to GPT?",
      "back": "1) Take the last token's hidden state (after all transformer layers). 2) Pass through Linear layer: d_model → num_classes. 3) For binary classification, use sigmoid; for multi-class, use softmax. The last token 'sees' the entire sequence via attention.",
      "concept": "finetuning",
      "tier": "mastery",
      "manningChapter": "6.5"
    },
    {
      "id": "32",
      "front": "Should you freeze layers during fine-tuning?",
      "back": "Depends on data size. Small dataset: freeze most layers, train only top layers + classification head. Large dataset: fine-tune all layers (but use smaller learning rate than pretraining). Embeddings are usually fine-tuned to adapt to domain-specific vocabulary.",
      "concept": "finetuning",
      "tier": "mastery",
      "manningChapter": "6.7"
    },
    {
      "id": "33",
      "front": "What is instruction fine-tuning?",
      "back": "Fine-tune on (instruction, response) pairs to make the model follow instructions. Example: (\"Translate to French: Hello\", \"Bonjour\"). Converts a completion model into an assistant model. Foundation for ChatGPT-style models (before RLHF).",
      "concept": "instruction-tuning",
      "tier": "mastery",
      "manningChapter": "7.1"
    },
    {
      "id": "34",
      "front": "What is the Alpaca instruction format?",
      "back": "Template: 'Below is an instruction... ### Instruction: {instruction} ### Input: {input} ### Response: {response}'. Standardized format helps the model recognize instruction-following tasks. Input is optional (empty for many tasks).",
      "concept": "instruction-tuning",
      "tier": "mastery",
      "manningChapter": "7.2"
    },
    {
      "id": "35",
      "front": "How is instruction tuning data prepared for training?",
      "back": "Concatenate instruction + input + response into single sequence. Mask loss on instruction/input (only backprop on response tokens). This way model learns to generate responses, not regurgitate instructions. Use special tokens to mark boundaries.",
      "concept": "instruction-tuning",
      "tier": "mastery",
      "manningChapter": "7.3"
    },
    {
      "id": "36",
      "front": "What evaluation metrics are used for instruction-tuned models?",
      "back": "No single metric works. Use: 1) Human evaluation (helpfulness, safety), 2) GPT-4 as judge (score responses), 3) Task-specific metrics (ROUGE for summarization), 4) Win rate vs baselines. Human eval is gold standard but expensive.",
      "concept": "instruction-tuning",
      "tier": "mastery",
      "manningChapter": "7.8"
    },
    {
      "id": "37",
      "front": "What is the context length and why does it matter?",
      "back": "Maximum sequence length the model can process (e.g., 2048 tokens for GPT-2). Limited by: 1) memory (attention is O(n²)), 2) positional encodings. Longer context = better understanding but slower + more memory. Models trained with specific context length.",
      "concept": "architecture",
      "tier": "intermediate",
      "manningChapter": "2.6"
    },
    {
      "id": "38",
      "front": "What causes the 'attention is all you need' revolution?",
      "back": "Transformers replaced RNNs/LSTMs by using attention for all sequence modeling. Benefits: 1) Parallelizable (RNNs are sequential), 2) Longer-range dependencies, 3) More interpretable (attention weights show what model focuses on), 4) Better gradient flow.",
      "concept": "architecture",
      "tier": "intermediate",
      "manningChapter": "3.1"
    },
    {
      "id": "39",
      "front": "What's the computational complexity of self-attention?",
      "back": "O(n² * d) where n is sequence length, d is embedding dimension. The n² comes from computing all pairwise attention scores. This is why very long sequences (>10k tokens) are expensive. Optimizations: sparse attention, linear attention, sliding windows.",
      "concept": "attention",
      "tier": "advanced",
      "manningChapter": "3.4"
    },
    {
      "id": "40",
      "front": "What is the difference between GPT and BERT architectures?",
      "back": "GPT: decoder-only, causal (left-to-right) attention, autoregressive generation. BERT: encoder-only, bidirectional attention, masked language modeling (predict masked tokens). GPT for generation, BERT for understanding. Modern trend: decoder-only for both (GPT-style).",
      "concept": "architecture",
      "tier": "advanced",
      "manningChapter": "4.1"
    },
    {
      "id": "41",
      "front": "Why use mixed precision training?",
      "back": "Train with float16 instead of float32 to: 1) Halve memory usage (fit larger batches), 2) Speed up compute on modern GPUs (Tensor Cores), 3) Maintain accuracy via loss scaling and float32 master weights. Critical for large models.",
      "concept": "training",
      "tier": "expert",
      "manningChapter": "5.2"
    },
    {
      "id": "42",
      "front": "What is gradient accumulation and when to use it?",
      "back": "Accumulate gradients over multiple forward passes before updating weights. Simulates larger batch sizes when GPU memory is limited. Example: 4 accumulation steps with batch_size=8 ≈ batch_size=32. Essential for training large models on consumer GPUs.",
      "concept": "training",
      "tier": "expert",
      "manningChapter": "5.2"
    },
    {
      "id": "43",
      "front": "What is perplexity and how is it calculated?",
      "back": "Perplexity = exp(cross_entropy_loss). Measures how 'surprised' the model is by test data. Lower is better. Perplexity of 50 means model is as uncertain as if choosing uniformly from 50 tokens. Widely used metric for language models.",
      "concept": "training",
      "tier": "expert",
      "manningChapter": "5.1.3"
    },
    {
      "id": "44",
      "front": "What is catastrophic forgetting in fine-tuning?",
      "back": "When fine-tuning on task A causes model to forget pretraining knowledge or previous task B. Mitigations: 1) Use small learning rate, 2) Mix in pretraining data, 3) Regularization toward pretrained weights, 4) Parameter-efficient fine-tuning (LoRA).",
      "concept": "finetuning",
      "tier": "mastery",
      "manningChapter": "6.7"
    },
    {
      "id": "45",
      "front": "What data was GPT-2 pretrained on?",
      "back": "WebText: 40GB of text from 8M web pages (URLs from Reddit with 3+ upvotes). Diverse, high-quality content. OpenAI didn't release the dataset. GPT-3 used even more diverse data (Common Crawl, books, Wikipedia, etc). Data quality matters more than quantity.",
      "concept": "pretraining",
      "tier": "expert",
      "manningChapter": "5.5"
    },
    {
      "id": "46",
      "front": "What comes after instruction tuning in the LLM training pipeline?",
      "back": "RLHF (Reinforcement Learning from Human Feedback). Steps: 1) Instruction tuning (supervised), 2) Train reward model on human preference rankings, 3) Optimize policy with PPO using reward model. This creates ChatGPT-style models. But RLHF is complex and unstable.",
      "concept": "instruction-tuning",
      "tier": "mastery",
      "manningChapter": "7.1"
    },
    {
      "id": "47",
      "front": "What is few-shot learning in LLMs?",
      "back": "Give the model examples of the task in the prompt without fine-tuning. Example: 'Translate: cat→chat, dog→chien, bird→?'. The model learns the pattern from examples. GPT-3 showed this emerges at scale. Instruction tuning improves few-shot abilities.",
      "concept": "prompting",
      "tier": "mastery",
      "manningChapter": "7.1"
    },
    {
      "id": "48",
      "front": "Why is tokenizer vocabulary size typically ~50k?",
      "back": "Tradeoff: smaller vocab = longer sequences (slow, worse long-range), larger vocab = huge embedding matrix (memory, rare tokens undertrained). 50k balances efficiency and coverage. BPE naturally reaches this size on English text. GPT uses 50,257 tokens.",
      "concept": "tokenization",
      "tier": "foundation",
      "manningChapter": "2.5"
    },
    {
      "id": "49",
      "front": "What is the curse of dimensionality in embeddings?",
      "back": "In high dimensions (e.g., 768), almost all points are far apart and equidistant. But this is fine for neural networks - they learn meaningful structure in high-D space that doesn't exist in low-D projections. Embeddings need high dimensions to capture complex semantics.",
      "concept": "embeddings",
      "tier": "foundation",
      "manningChapter": "2.7"
    },
    {
      "id": "50",
      "front": "How do you verify a loaded pretrained model is correct?",
      "back": "1) Check parameter count matches. 2) Forward pass with same input as reference implementation. 3) Compare output logits/probabilities (should match exactly). 4) Generate text with same prompt and seed - should produce identical output. Any difference means wrong weight mapping.",
      "concept": "model-management",
      "tier": "expert",
      "manningChapter": "5.5"
    }
  ],
  "stats": {
    "total": 50,
    "byTier": {
      "foundation": 8,
      "intermediate": 9,
      "advanced": 10,
      "expert": 13,
      "mastery": 10
    },
    "byConcept": {
      "tokenization": 4,
      "embeddings": 3,
      "data-preparation": 2,
      "attention": 8,
      "multi-head-attention": 2,
      "architecture": 8,
      "generation": 4,
      "training": 8,
      "model-management": 3,
      "finetuning": 4,
      "instruction-tuning": 4
    }
  }
}
